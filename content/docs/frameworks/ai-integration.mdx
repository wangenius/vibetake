---
title: AI é›†æˆ
description: ä½¿ç”¨ Vercel AI SDK é›†æˆ AI åŠŸèƒ½
---

# Vercel AI SDK é›†æˆæŒ‡å—

Vibetake é›†æˆäº† [Vercel AI SDK](https://sdk.vercel.ai)ï¼Œä¸ºä½ çš„åº”ç”¨æä¾›å¼ºå¤§çš„ AI åŠŸèƒ½ã€‚æœ¬æ–‡æ¡£å°†è¯¦ç»†ä»‹ç»å¦‚ä½•é…ç½®å’Œä½¿ç”¨ AI æœåŠ¡ã€‚

## æ¦‚è¿°

Vercel AI SDK æ˜¯ä¸€ä¸ªå¼€æºåº“ï¼Œæä¾›äº†ä¸å¤šç§ AI æ¨¡å‹äº¤äº’çš„ç»Ÿä¸€æ¥å£ã€‚å®ƒæ”¯æŒæµå¼å“åº”ã€å·¥å…·è°ƒç”¨ã€ç»“æ„åŒ–è¾“å‡ºç­‰é«˜çº§åŠŸèƒ½ã€‚

### æ ¸å¿ƒç‰¹æ€§

- ğŸ¤– **å¤šæ¨¡å‹æ”¯æŒ** - OpenAIã€Anthropicã€Googleã€Cohere ç­‰
- ğŸ’¬ **æµå¼èŠå¤©** - å®æ—¶å“åº”å’Œæ‰“å­—æ•ˆæœ
- ğŸ”§ **å·¥å…·è°ƒç”¨** - å‡½æ•°è°ƒç”¨å’Œå¤–éƒ¨ API é›†æˆ
- ğŸ“ **æ–‡æœ¬ç”Ÿæˆ** - æ™ºèƒ½å†…å®¹åˆ›ä½œå’Œè¡¥å…¨
- ğŸ–¼ï¸ **å¤šæ¨¡æ€** - å›¾åƒåˆ†æå’Œç”Ÿæˆ
- ğŸ¯ **ç»“æ„åŒ–è¾“å‡º** - JSON æ ¼å¼çš„ç»“æ„åŒ–æ•°æ®ç”Ÿæˆ
- âš¡ **è¾¹ç¼˜è¿è¡Œæ—¶** - ä¼˜åŒ–çš„æ€§èƒ½å’Œå»¶è¿Ÿ

## ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    A[å®¢æˆ·ç«¯ç»„ä»¶] --> B[AI Hooks]
    B --> C[API è·¯ç”±]
    C --> D[AI SDK Core]
    D --> E[æ¨¡å‹æä¾›å•†]
    
    subgraph "AI æœåŠ¡æµç¨‹"
        F[ç”¨æˆ·è¾“å…¥] --> G[è¯·æ±‚å¤„ç†]
        G --> H[æ¨¡å‹è°ƒç”¨]
        H --> I[æµå¼å“åº”]
        I --> J[å®¢æˆ·ç«¯æ¸²æŸ“]
    end
    
    subgraph "æ”¯æŒçš„æä¾›å•†"
        K[OpenAI]
        L[Anthropic]
        M[Google AI]
        N[Cohere]
        O[æœ¬åœ°æ¨¡å‹]
    end
```

## å®‰è£…å’Œé…ç½®

### 1. å®‰è£…ä¾èµ–

```bash
# å®‰è£… Vercel AI SDK
npm install ai @ai-sdk/openai @ai-sdk/anthropic @ai-sdk/google

# æˆ–ä½¿ç”¨å…¶ä»–æä¾›å•†
npm install @ai-sdk/cohere @ai-sdk/mistral
```

### 2. ç¯å¢ƒå˜é‡é…ç½®

åœ¨ `.env.local` æ–‡ä»¶ä¸­é…ç½® API å¯†é’¥ï¼š

```bash
# OpenAI é…ç½®
OPENAI_API_KEY=sk-...

# Anthropic é…ç½®
ANTHROPIC_API_KEY=sk-ant-...

# Google AI é…ç½®
GOOGLE_GENERATIVE_AI_API_KEY=...

# Cohere é…ç½®
COHERE_API_KEY=...

# è‡ªå®šä¹‰é…ç½®
AI_MODEL_PROVIDER=openai
AI_DEFAULT_MODEL=gpt-4-turbo
```

## å¿«é€Ÿå¼€å§‹

### 1. åˆ›å»º AI æœåŠ¡é…ç½®

é¦–å…ˆåˆ›å»º AI æœåŠ¡çš„é…ç½®æ–‡ä»¶ï¼š

```typescript
// src/services/ai/config.ts
import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';
import { google } from '@ai-sdk/google';

// æ¨¡å‹é…ç½®
export const models = {
  // OpenAI æ¨¡å‹
  'gpt-4-turbo': openai('gpt-4-turbo'),
  'gpt-4': openai('gpt-4'),
  'gpt-3.5-turbo': openai('gpt-3.5-turbo'),
  
  // Anthropic æ¨¡å‹
  'claude-3-opus': anthropic('claude-3-opus-20240229'),
  'claude-3-sonnet': anthropic('claude-3-sonnet-20240229'),
  'claude-3-haiku': anthropic('claude-3-haiku-20240307'),
  
  // Google æ¨¡å‹
  'gemini-pro': google('models/gemini-pro'),
  'gemini-pro-vision': google('models/gemini-pro-vision'),
};

// é»˜è®¤æ¨¡å‹é…ç½®
export const defaultModel = models[process.env.AI_DEFAULT_MODEL as keyof typeof models] || models['gpt-4-turbo'];

// æ¨¡å‹é€‰æ‹©å™¨
export function getModel(modelName: string) {
  return models[modelName as keyof typeof models] || defaultModel;
}
```

### 2. åˆ›å»ºèŠå¤© API è·¯ç”±

```typescript
// src/app/api/chat/route.ts
import { streamText, convertToCoreMessages } from 'ai';
import { getModel } from '@/services/ai/config';
import { auth } from '@/services/userauth/auth';
import { headers } from 'next/headers';

export async function POST(req: Request) {
  try {
    // éªŒè¯ç”¨æˆ·èº«ä»½
    const session = await auth.api.getSession({
      headers: headers(),
    });

    if (!session) {
      return new Response('Unauthorized', { status: 401 });
    }

    const { messages, model = 'gpt-4-turbo' } = await req.json();

    // éªŒè¯æ¶ˆæ¯æ ¼å¼
    if (!messages || !Array.isArray(messages)) {
      return new Response('Invalid messages format', { status: 400 });
    }

    // è½¬æ¢æ¶ˆæ¯æ ¼å¼
    const coreMessages = convertToCoreMessages(messages);

    // ç”Ÿæˆæµå¼å“åº”
    const result = await streamText({
      model: getModel(model),
      messages: coreMessages,
      maxTokens: 1000,
      temperature: 0.7,
      // ç³»ç»Ÿæç¤º
      system: 'ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ AI åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚',
    });

    return result.toDataStreamResponse();
  } catch (error) {
    console.error('Chat API error:', error);
    return new Response('Internal Server Error', { status: 500 });
  }
}
```

### 3. åˆ›å»ºèŠå¤©ç»„ä»¶

```tsx
// src/components/ai/chat.tsx
'use client';

import { useChat } from 'ai/react';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { ScrollArea } from '@/components/ui/scroll-area';
import { Avatar, AvatarFallback } from '@/components/ui/avatar';
import { Send, Bot, User } from 'lucide-react';

export function ChatComponent() {
  const { 
    messages, 
    input, 
    handleInputChange, 
    handleSubmit, 
    isLoading,
    error 
  } = useChat({
    api: '/api/chat',
    onError: (error) => {
      console.error('Chat error:', error);
    },
  });

  return (
    <Card className="w-full max-w-2xl mx-auto h-[600px] flex flex-col">
      <CardHeader>
        <CardTitle className="flex items-center gap-2">
          <Bot className="w-5 h-5" />
          AI åŠ©æ‰‹
        </CardTitle>
      </CardHeader>
      
      <CardContent className="flex-1 flex flex-col gap-4">
        <ScrollArea className="flex-1 pr-4">
          <div className="space-y-4">
            {messages.map((message) => (
              <div
                key={message.id}
                className={`flex gap-3 ${
                  message.role === 'user' ? 'justify-end' : 'justify-start'
                }`}
              >
                <div className={`flex gap-3 max-w-[80%] ${
                  message.role === 'user' ? 'flex-row-reverse' : 'flex-row'
                }`}>
                  <Avatar className="w-8 h-8">
                    <AvatarFallback>
                      {message.role === 'user' ? (
                        <User className="w-4 h-4" />
                      ) : (
                        <Bot className="w-4 h-4" />
                      )}
                    </AvatarFallback>
                  </Avatar>
                  
                  <div className={`rounded-lg px-3 py-2 ${
                    message.role === 'user'
                      ? 'bg-primary text-primary-foreground'
                      : 'bg-muted'
                  }`}>
                    <p className="text-sm whitespace-pre-wrap">
                      {message.content}
                    </p>
                  </div>
                </div>
              </div>
            ))}
            
            {isLoading && (
              <div className="flex gap-3 justify-start">
                <Avatar className="w-8 h-8">
                  <AvatarFallback>
                    <Bot className="w-4 h-4" />
                  </AvatarFallback>
                </Avatar>
                <div className="bg-muted rounded-lg px-3 py-2">
                  <div className="flex space-x-1">
                    <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce"></div>
                    <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" style={{ animationDelay: '0.1s' }}></div>
                    <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" style={{ animationDelay: '0.2s' }}></div>
                  </div>
                </div>
              </div>
            )}
          </div>
        </ScrollArea>

        {error && (
          <div className="p-3 text-sm text-red-600 bg-red-50 border border-red-200 rounded-lg">
            å‘ç”Ÿé”™è¯¯ï¼š{error.message}
          </div>
        )}

        <form onSubmit={handleSubmit} className="flex gap-2">
          <Input
            value={input}
            onChange={handleInputChange}
            placeholder="è¾“å…¥ä½ çš„é—®é¢˜..."
            disabled={isLoading}
            className="flex-1"
          />
          <Button type="submit" disabled={isLoading || !input.trim()}>
            <Send className="w-4 h-4" />
          </Button>
        </form>
      </CardContent>
    </Card>
  );
}
```

## æ ¸å¿ƒåŠŸèƒ½

### 1. æ–‡æœ¬ç”Ÿæˆ

```typescript
// src/services/ai/text-generation.ts
import { generateText } from 'ai';
import { getModel } from './config';

export async function generateContent(
  prompt: string,
  options: {
    model?: string;
    maxTokens?: number;
    temperature?: number;
    system?: string;
  } = {}
) {
  const {
    model = 'gpt-4-turbo',
    maxTokens = 1000,
    temperature = 0.7,
    system = 'ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ AI åŠ©æ‰‹ã€‚'
  } = options;

  try {
    const { text, usage } = await generateText({
      model: getModel(model),
      prompt,
      system,
      maxTokens,
      temperature,
    });

    return {
      success: true,
      text,
      usage,
    };
  } catch (error) {
    console.error('Text generation error:', error);
    return {
      success: false,
      error: error instanceof Error ? error.message : 'Unknown error',
    };
  }
}

// ä½¿ç”¨ç¤ºä¾‹
const result = await generateContent(
  'å†™ä¸€ç¯‡å…³äº React 18 æ–°ç‰¹æ€§çš„æŠ€æœ¯æ–‡ç« å¤§çº²',
  {
    model: 'gpt-4-turbo',
    maxTokens: 800,
    temperature: 0.8,
  }
);

if (result.success) {
  console.log('ç”Ÿæˆçš„å†…å®¹:', result.text);
  console.log('ä½¿ç”¨æƒ…å†µ:', result.usage);
} else {
  console.error('ç”Ÿæˆå¤±è´¥:', result.error);
}
```

### 2. å·¥å…·è°ƒç”¨ (Function Calling)

å·¥å…·è°ƒç”¨å…è®¸ AI æ¨¡å‹æ‰§è¡Œå¤–éƒ¨å‡½æ•°ï¼Œè·å–å®æ—¶æ•°æ®æˆ–æ‰§è¡Œç‰¹å®šæ“ä½œï¼š

```typescript
// src/services/ai/tools.ts
import { generateText, tool } from 'ai';
import { getModel } from './config';
import { z } from 'zod';

// å®šä¹‰å·¥å…·
export const weatherTool = tool({
  description: 'è·å–æŒ‡å®šåŸå¸‚çš„å¤©æ°”ä¿¡æ¯',
  parameters: z.object({
    city: z.string().describe('åŸå¸‚åç§°'),
    unit: z.enum(['celsius', 'fahrenheit']).optional().describe('æ¸©åº¦å•ä½'),
  }),
  execute: async ({ city, unit = 'celsius' }) => {
    // è¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„å¤©æ°” API
    const mockWeatherData = {
      city,
      temperature: unit === 'celsius' ? 25 : 77,
      condition: 'æ™´å¤©',
      humidity: 60,
      windSpeed: 10,
    };
    
    return JSON.stringify(mockWeatherData);
  },
});

export const calculatorTool = tool({
  description: 'æ‰§è¡Œæ•°å­¦è®¡ç®—',
  parameters: z.object({
    expression: z.string().describe('æ•°å­¦è¡¨è¾¾å¼ï¼Œå¦‚ "2 + 3 * 4"'),
  }),
  execute: async ({ expression }) => {
    try {
      // å®‰å…¨çš„æ•°å­¦è¡¨è¾¾å¼è®¡ç®—
      const result = Function(`"use strict"; return (${expression})`)();
      return `è®¡ç®—ç»“æœ: ${expression} = ${result}`;
    } catch (error) {
      return `è®¡ç®—é”™è¯¯: æ— æ•ˆçš„è¡¨è¾¾å¼ "${expression}"`;
    }
  },
});

// ä½¿ç”¨å·¥å…·çš„èŠå¤© API
export async function chatWithTools(
  messages: any[],
  availableTools: string[] = ['weather', 'calculator']
) {
  const tools: Record<string, any> = {};
  
  if (availableTools.includes('weather')) {
    tools.getWeather = weatherTool;
  }
  
  if (availableTools.includes('calculator')) {
    tools.calculate = calculatorTool;
  }

  const { text, toolCalls, toolResults } = await generateText({
    model: getModel('gpt-4-turbo'),
    messages,
    tools,
    maxToolRoundtrips: 3, // æœ€å¤šæ‰§è¡Œ3è½®å·¥å…·è°ƒç”¨
  });

  return {
    text,
    toolCalls,
    toolResults,
  };
}
```

### 3. ç»“æ„åŒ–è¾“å‡º

ç”Ÿæˆç¬¦åˆç‰¹å®š schema çš„ç»“æ„åŒ–æ•°æ®ï¼š

```typescript
// src/services/ai/structured-output.ts
import { generateObject } from 'ai';
import { getModel } from './config';
import { z } from 'zod';

// å®šä¹‰æ•°æ®ç»“æ„
export const articleSchema = z.object({
  title: z.string().describe('æ–‡ç« æ ‡é¢˜'),
  summary: z.string().describe('æ–‡ç« æ‘˜è¦'),
  sections: z.array(z.object({
    heading: z.string().describe('ç« èŠ‚æ ‡é¢˜'),
    content: z.string().describe('ç« èŠ‚å†…å®¹'),
    keyPoints: z.array(z.string()).describe('å…³é”®è¦ç‚¹'),
  })).describe('æ–‡ç« ç« èŠ‚'),
  tags: z.array(z.string()).describe('æ–‡ç« æ ‡ç­¾'),
  estimatedReadTime: z.number().describe('é¢„è®¡é˜…è¯»æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰'),
});

export const userProfileSchema = z.object({
  name: z.string(),
  age: z.number(),
  interests: z.array(z.string()),
  skills: z.array(z.object({
    name: z.string(),
    level: z.enum(['beginner', 'intermediate', 'advanced']),
  })),
  bio: z.string(),
});

// ç”Ÿæˆç»“æ„åŒ–æ–‡ç« 
export async function generateArticle(topic: string) {
  const { object } = await generateObject({
    model: getModel('gpt-4-turbo'),
    schema: articleSchema,
    prompt: `å†™ä¸€ç¯‡å…³äº"${topic}"çš„æŠ€æœ¯æ–‡ç« ï¼ŒåŒ…å«è¯¦ç»†çš„ç« èŠ‚å’Œè¦ç‚¹ã€‚`,
  });

  return object;
}

// åˆ†æç”¨æˆ·ç®€ä»‹
export async function analyzeUserProfile(profileText: string) {
  const { object } = await generateObject({
    model: getModel('gpt-4-turbo'),
    schema: userProfileSchema,
    prompt: `åˆ†æä»¥ä¸‹ç”¨æˆ·ç®€ä»‹ï¼Œæå–ç»“æ„åŒ–ä¿¡æ¯ï¼š\n\n${profileText}`,
  });

  return object;
}
```

### 4. å›¾åƒåˆ†æ

å¤„ç†å’Œåˆ†æå›¾åƒå†…å®¹ï¼š

```typescript
// src/services/ai/image-analysis.ts
import { generateObject } from 'ai';
import { getModel } from './config';
import { z } from 'zod';

export const imageAnalysisSchema = z.object({
  description: z.string().describe('å›¾åƒçš„è¯¦ç»†æè¿°'),
  objects: z.array(z.object({
    name: z.string().describe('ç‰©ä½“åç§°'),
    confidence: z.number().describe('è¯†åˆ«ç½®ä¿¡åº¦ 0-1'),
    position: z.string().describe('ç‰©ä½“åœ¨å›¾åƒä¸­çš„ä½ç½®'),
  })).describe('è¯†åˆ«åˆ°çš„ç‰©ä½“'),
  colors: z.array(z.string()).describe('ä¸»è¦é¢œè‰²'),
  mood: z.string().describe('å›¾åƒçš„æƒ…æ„Ÿè‰²è°ƒ'),
  tags: z.array(z.string()).describe('ç›¸å…³æ ‡ç­¾'),
  isAppropriate: z.boolean().describe('å†…å®¹æ˜¯å¦åˆé€‚'),
});

export async function analyzeImage(imageUrl: string) {
  const { object } = await generateObject({
    model: getModel('gemini-pro-vision'), // ä½¿ç”¨æ”¯æŒè§†è§‰çš„æ¨¡å‹
    schema: imageAnalysisSchema,
    messages: [
      {
        role: 'user',
        content: [
          { type: 'text', text: 'è¯·åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹' },
          { type: 'image', image: imageUrl },
        ],
      },
    ],
  });

  return object;
}

// å›¾åƒå†…å®¹å®¡æ ¸
export async function moderateImage(imageUrl: string) {
  const analysis = await analyzeImage(imageUrl);
  
  return {
    isAppropriate: analysis.isAppropriate,
    reason: analysis.isAppropriate ? 'å†…å®¹åˆé€‚' : 'å†…å®¹å¯èƒ½ä¸åˆé€‚',
    details: analysis,
  };
}
```

## é«˜çº§åŠŸèƒ½

### 1. æµå¼æ–‡æœ¬è¡¥å…¨

```tsx
// src/components/ai/text-completion.tsx
'use client';

import { useCompletion } from 'ai/react';
import { Button } from '@/components/ui/button';
import { Textarea } from '@/components/ui/textarea';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';

export function TextCompletion() {
  const {
    completion,
    input,
    handleInputChange,
    handleSubmit,
    isLoading,
    stop,
  } = useCompletion({
    api: '/api/completion',
  });

  return (
    <Card className="w-full max-w-2xl mx-auto">
      <CardHeader>
        <CardTitle>æ™ºèƒ½æ–‡æœ¬è¡¥å…¨</CardTitle>
      </CardHeader>
      <CardContent className="space-y-4">
        <form onSubmit={handleSubmit} className="space-y-4">
          <Textarea
            value={input}
            onChange={handleInputChange}
            placeholder="è¾“å…¥æ–‡æœ¬å¼€å¤´ï¼ŒAI å°†å¸®ä½ ç»­å†™..."
            rows={4}
          />
          <div className="flex gap-2">
            <Button type="submit" disabled={isLoading}>
              {isLoading ? 'ç”Ÿæˆä¸­...' : 'å¼€å§‹è¡¥å…¨'}
            </Button>
            {isLoading && (
              <Button type="button" variant="outline" onClick={stop}>
                åœæ­¢
              </Button>
            )}
          </div>
        </form>

        {completion && (
          <div className="mt-4">
            <h3 className="font-semibold mb-2">AI ç»­å†™å†…å®¹ï¼š</h3>
            <div className="p-4 bg-muted rounded-lg">
              <p className="whitespace-pre-wrap">{completion}</p>
            </div>
          </div>
        )}
      </CardContent>
    </Card>
  );
}
```

### 2. å¤šè½®å¯¹è¯ç®¡ç†

```typescript
// src/services/ai/conversation.ts
import { generateText } from 'ai';
import { getModel } from './config';

export interface ConversationMessage {
  id: string;
  role: 'user' | 'assistant' | 'system';
  content: string;
  timestamp: Date;
  metadata?: Record<string, any>;
}

export class ConversationManager {
  private messages: ConversationMessage[] = [];
  private maxMessages: number = 20; // æœ€å¤§æ¶ˆæ¯æ•°é‡
  private systemPrompt: string;

  constructor(systemPrompt: string = 'ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„ AI åŠ©æ‰‹ã€‚') {
    this.systemPrompt = systemPrompt;
  }

  addMessage(role: 'user' | 'assistant', content: string, metadata?: Record<string, any>) {
    const message: ConversationMessage = {
      id: crypto.randomUUID(),
      role,
      content,
      timestamp: new Date(),
      metadata,
    };

    this.messages.push(message);

    // ä¿æŒæ¶ˆæ¯æ•°é‡åœ¨é™åˆ¶å†…
    if (this.messages.length > this.maxMessages) {
      this.messages = this.messages.slice(-this.maxMessages);
    }

    return message;
  }

  async generateResponse(userMessage: string, model: string = 'gpt-4-turbo') {
    // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    this.addMessage('user', userMessage);

    // å‡†å¤‡æ¶ˆæ¯å†å²
    const messages = [
      { role: 'system' as const, content: this.systemPrompt },
      ...this.messages.map(msg => ({
        role: msg.role as 'user' | 'assistant',
        content: msg.content,
      })),
    ];

    try {
      const { text, usage } = await generateText({
        model: getModel(model),
        messages,
        maxTokens: 1000,
        temperature: 0.7,
      });

      // æ·»åŠ  AI å“åº”
      const assistantMessage = this.addMessage('assistant', text, { usage });

      return {
        success: true,
        message: assistantMessage,
        usage,
      };
    } catch (error) {
      console.error('Conversation error:', error);
      return {
        success: false,
        error: error instanceof Error ? error.message : 'Unknown error',
      };
    }
  }

  getMessages() {
    return [...this.messages];
  }

  clearHistory() {
    this.messages = [];
  }

  exportConversation() {
    return {
      systemPrompt: this.systemPrompt,
      messages: this.messages,
      exportedAt: new Date(),
    };
  }
}
```

### 3. AI åŠ©æ‰‹é›†æˆ

```tsx
// src/components/ai/ai-assistant.tsx
'use client';

import { useState } from 'react';
import { ConversationManager } from '@/services/ai/conversation';
import { Button } from '@/components/ui/button';
import { Input } from '@/components/ui/input';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from '@/components/ui/select';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';

const AI_MODELS = [
  { value: 'gpt-4-turbo', label: 'GPT-4 Turbo' },
  { value: 'gpt-4', label: 'GPT-4' },
  { value: 'claude-3-opus', label: 'Claude 3 Opus' },
  { value: 'claude-3-sonnet', label: 'Claude 3 Sonnet' },
  { value: 'gemini-pro', label: 'Gemini Pro' },
];

export function AIAssistant() {
  const [conversation] = useState(() => new ConversationManager());
  const [messages, setMessages] = useState(conversation.getMessages());
  const [input, setInput] = useState('');
  const [selectedModel, setSelectedModel] = useState('gpt-4-turbo');
  const [isLoading, setIsLoading] = useState(false);

  const handleSubmit = async (e: React.FormEvent) => {
    e.preventDefault();
    if (!input.trim() || isLoading) return;

    setIsLoading(true);
    const userInput = input;
    setInput('');

    try {
      const result = await conversation.generateResponse(userInput, selectedModel);
      
      if (result.success) {
        setMessages(conversation.getMessages());
      } else {
        console.error('AI response error:', result.error);
      }
    } catch (error) {
      console.error('Submit error:', error);
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <Card className="w-full max-w-4xl mx-auto h-[700px] flex flex-col">
      <CardHeader>
        <div className="flex items-center justify-between">
          <CardTitle>AI æ™ºèƒ½åŠ©æ‰‹</CardTitle>
          <div className="flex items-center gap-2">
            <Select value={selectedModel} onValueChange={setSelectedModel}>
              <SelectTrigger className="w-40">
                <SelectValue />
              </SelectTrigger>
              <SelectContent>
                {AI_MODELS.map((model) => (
                  <SelectItem key={model.value} value={model.value}>
                    {model.label}
                  </SelectItem>
                ))}
              </SelectContent>
            </Select>
            <Button
              variant="outline"
              size="sm"
              onClick={() => {
                conversation.clearHistory();
                setMessages([]);
              }}
            >
              æ¸…ç©ºå¯¹è¯
            </Button>
          </div>
        </div>
      </CardHeader>

      <CardContent className="flex-1 flex flex-col gap-4">
        <div className="flex-1 overflow-y-auto space-y-4">
          {messages.map((message) => (
            <div
              key={message.id}
              className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'}`}
            >
              <div className={`max-w-[80%] rounded-lg p-3 ${
                message.role === 'user'
                  ? 'bg-primary text-primary-foreground'
                  : 'bg-muted'
              }`}>
                <div className="flex items-center gap-2 mb-1">
                  <Badge variant="secondary" className="text-xs">
                    {message.role === 'user' ? 'ç”¨æˆ·' : 'AI'}
                  </Badge>
                  <span className="text-xs text-muted-foreground">
                    {message.timestamp.toLocaleTimeString()}
                  </span>
                </div>
                <p className="whitespace-pre-wrap">{message.content}</p>
                {message.metadata?.usage && (
                  <div className="text-xs text-muted-foreground mt-2">
                    Tokens: {message.metadata.usage.totalTokens}
                  </div>
                )}
              </div>
            </div>
          ))}
          
          {isLoading && (
            <div className="flex justify-start">
              <div className="bg-muted rounded-lg p-3">
                <div className="flex items-center gap-2">
                  <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce"></div>
                  <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" style={{ animationDelay: '0.1s' }}></div>
                  <div className="w-2 h-2 bg-gray-400 rounded-full animate-bounce" style={{ animationDelay: '0.2s' }}></div>
                </div>
              </div>
            </div>
          )}
        </div>

        <form onSubmit={handleSubmit} className="flex gap-2">
          <Input
            value={input}
            onChange={(e) => setInput(e.target.value)}
            placeholder="è¾“å…¥ä½ çš„é—®é¢˜..."
            disabled={isLoading}
            className="flex-1"
          />
          <Button type="submit" disabled={isLoading || !input.trim()}>
            å‘é€
          </Button>
        </form>
      </CardContent>
    </Card>
  );
}
```

## æ€§èƒ½ä¼˜åŒ–

### 1. è¯·æ±‚ç¼“å­˜

```typescript
// src/services/ai/cache.ts
interface CacheEntry {
  result: any;
  timestamp: number;
  ttl: number;
}

class AICache {
  private cache = new Map<string, CacheEntry>();

  set(key: string, value: any, ttlMs: number = 300000) { // é»˜è®¤5åˆ†é’Ÿ
    this.cache.set(key, {
      result: value,
      timestamp: Date.now(),
      ttl: ttlMs,
    });
  }

  get(key: string): any | null {
    const entry = this.cache.get(key);
    if (!entry) return null;

    if (Date.now() - entry.timestamp > entry.ttl) {
      this.cache.delete(key);
      return null;
    }

    return entry.result;
  }

  clear() {
    this.cache.clear();
  }

  // ç”Ÿæˆç¼“å­˜é”®
  generateKey(prompt: string, model: string, options: any = {}): string {
    return `${model}:${JSON.stringify({ prompt, ...options })}`;
  }
}

export const aiCache = new AICache();

// å¸¦ç¼“å­˜çš„æ–‡æœ¬ç”Ÿæˆ
export async function generateTextWithCache(
  prompt: string,
  options: any = {}
) {
  const cacheKey = aiCache.generateKey(prompt, options.model || 'gpt-4-turbo', options);
  
  // å°è¯•ä»ç¼“å­˜è·å–
  const cached = aiCache.get(cacheKey);
  if (cached) {
    return { ...cached, fromCache: true };
  }

  // ç”Ÿæˆæ–°å†…å®¹
  const result = await generateContent(prompt, options);
  
  // ç¼“å­˜ç»“æœ
  if (result.success) {
    aiCache.set(cacheKey, result);
  }

  return { ...result, fromCache: false };
}
```

### 2. é€Ÿç‡é™åˆ¶

```typescript
// src/services/ai/rate-limiter.ts
class RateLimiter {
  private requests = new Map<string, number[]>();
  private maxRequests: number;
  private windowMs: number;

  constructor(maxRequests: number = 10, windowMs: number = 60000) {
    this.maxRequests = maxRequests;
    this.windowMs = windowMs;
  }

  async checkLimit(userId: string): Promise<boolean> {
    const now = Date.now();
    const userRequests = this.requests.get(userId) || [];
    
    // æ¸…ç†è¿‡æœŸçš„è¯·æ±‚è®°å½•
    const validRequests = userRequests.filter(
      timestamp => now - timestamp < this.windowMs
    );

    if (validRequests.length >= this.maxRequests) {
      return false; // è¶…å‡ºé™åˆ¶
    }

    // è®°å½•æ–°è¯·æ±‚
    validRequests.push(now);
    this.requests.set(userId, validRequests);
    
    return true; // å…è®¸è¯·æ±‚
  }

  getRemainingRequests(userId: string): number {
    const now = Date.now();
    const userRequests = this.requests.get(userId) || [];
    const validRequests = userRequests.filter(
      timestamp => now - timestamp < this.windowMs
    );
    
    return Math.max(0, this.maxRequests - validRequests.length);
  }
}

export const rateLimiter = new RateLimiter(20, 60000); // æ¯åˆ†é’Ÿ20æ¬¡è¯·æ±‚
```

## å®‰å…¨å’Œæœ€ä½³å®è·µ

### 1. è¾“å…¥éªŒè¯å’Œæ¸…ç†

```typescript
// src/services/ai/security.ts
import { z } from 'zod';

// è¾“å…¥éªŒè¯ schema
export const chatInputSchema = z.object({
  message: z.string()
    .min(1, 'æ¶ˆæ¯ä¸èƒ½ä¸ºç©º')
    .max(4000, 'æ¶ˆæ¯é•¿åº¦ä¸èƒ½è¶…è¿‡4000å­—ç¬¦')
    .refine(
      (msg) => !containsHarmfulContent(msg),
      'æ¶ˆæ¯åŒ…å«ä¸å½“å†…å®¹'
    ),
  model: z.string().optional(),
  temperature: z.number().min(0).max(2).optional(),
  maxTokens: z.number().min(1).max(4000).optional(),
});

// æ£€æŸ¥æœ‰å®³å†…å®¹
function containsHarmfulContent(text: string): boolean {
  const harmfulPatterns = [
    /æš´åŠ›|è¡€è…¥|ææ€–/i,
    /è‰²æƒ…|æˆäºº|ä¸é›…/i,
    /ä»‡æ¨|æ­§è§†|åè§/i,
    // æ·»åŠ æ›´å¤šæ¨¡å¼...
  ];

  return harmfulPatterns.some(pattern => pattern.test(text));
}

// æ¸…ç†è¾“å‡ºå†…å®¹
export function sanitizeOutput(text: string): string {
  // ç§»é™¤æ½œåœ¨çš„æ¶æ„å†…å®¹
  return text
    .replace(/<script\b[^<]*(?:(?!<\/script>)<[^<]*)*<\/script>/gi, '')
    .replace(/javascript:/gi, '')
    .replace(/on\w+\s*=/gi, '');
}

// å†…å®¹å®¡æ ¸
export async function moderateContent(text: string): Promise<{
  isAppropriate: boolean;
  reason?: string;
  confidence: number;
}> {
  // è¿™é‡Œå¯ä»¥é›†æˆç¬¬ä¸‰æ–¹å†…å®¹å®¡æ ¸æœåŠ¡
  // å¦‚ OpenAI Moderation API
  
  const isAppropriate = !containsHarmfulContent(text);
  
  return {
    isAppropriate,
    reason: isAppropriate ? undefined : 'å†…å®¹å¯èƒ½åŒ…å«ä¸å½“ä¿¡æ¯',
    confidence: 0.8,
  };
}
```

### 2. æˆæœ¬ç›‘æ§

```typescript
// src/services/ai/cost-monitor.ts
interface UsageRecord {
  userId: string;
  model: string;
  inputTokens: number;
  outputTokens: number;
  cost: number;
  timestamp: Date;
}

class CostMonitor {
  private usage: UsageRecord[] = [];
  
  // æ¨¡å‹å®šä»·ï¼ˆæ¯1000 tokensçš„ä»·æ ¼ï¼Œå•ä½ï¼šç¾å…ƒï¼‰
  private pricing = {
    'gpt-4-turbo': { input: 0.01, output: 0.03 },
    'gpt-4': { input: 0.03, output: 0.06 },
    'gpt-3.5-turbo': { input: 0.001, output: 0.002 },
    'claude-3-opus': { input: 0.015, output: 0.075 },
    'claude-3-sonnet': { input: 0.003, output: 0.015 },
  };

  recordUsage(
    userId: string,
    model: string,
    inputTokens: number,
    outputTokens: number
  ) {
    const modelPricing = this.pricing[model as keyof typeof this.pricing];
    if (!modelPricing) return;

    const cost = 
      (inputTokens / 1000) * modelPricing.input +
      (outputTokens / 1000) * modelPricing.output;

    const record: UsageRecord = {
      userId,
      model,
      inputTokens,
      outputTokens,
      cost,
      timestamp: new Date(),
    };

    this.usage.push(record);
  }

  getUserCost(userId: string, days: number = 30): number {
    const cutoff = new Date();
    cutoff.setDate(cutoff.getDate() - days);

    return this.usage
      .filter(record => 
        record.userId === userId && 
        record.timestamp >= cutoff
      )
      .reduce((total, record) => total + record.cost, 0);
  }

  getTotalCost(days: number = 30): number {
    const cutoff = new Date();
    cutoff.setDate(cutoff.getDate() - days);

    return this.usage
      .filter(record => record.timestamp >= cutoff)
      .reduce((total, record) => total + record.cost, 0);
  }

  getUsageStats(days: number = 30) {
    const cutoff = new Date();
    cutoff.setDate(cutoff.getDate() - days);

    const recentUsage = this.usage.filter(
      record => record.timestamp >= cutoff
    );

    const stats = {
      totalRequests: recentUsage.length,
      totalCost: recentUsage.reduce((sum, r) => sum + r.cost, 0),
      totalTokens: recentUsage.reduce((sum, r) => sum + r.inputTokens + r.outputTokens, 0),
      modelUsage: {} as Record<string, number>,
    };

    recentUsage.forEach(record => {
      stats.modelUsage[record.model] = (stats.modelUsage[record.model] || 0) + 1;
    });

    return stats;
  }
}

export const costMonitor = new CostMonitor();
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **API å¯†é’¥é”™è¯¯**
   - æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®
   - ç¡®è®¤å¯†é’¥æ ¼å¼æ­£ç¡®
   - éªŒè¯å¯†é’¥æƒé™

2. **è¯·æ±‚è¶…æ—¶**
   - å¢åŠ è¶…æ—¶æ—¶é—´è®¾ç½®
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - è€ƒè™‘ä½¿ç”¨é‡è¯•æœºåˆ¶

3. **Token é™åˆ¶**
   - ä¼˜åŒ–æç¤ºè¯é•¿åº¦
   - ä½¿ç”¨é€‚å½“çš„ maxTokens è®¾ç½®
   - è€ƒè™‘åˆ†æ®µå¤„ç†é•¿æ–‡æœ¬

4. **æˆæœ¬è¿‡é«˜**
   - å®æ–½ç¼“å­˜ç­–ç•¥
   - ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
   - ä¼˜åŒ–æç¤ºè¯æ•ˆç‡

### è°ƒè¯•æŠ€å·§

```typescript
// å¯ç”¨è¯¦ç»†æ—¥å¿—
const debugMode = process.env.NODE_ENV === 'development';

if (debugMode) {
  console.log('AI Request:', {
    model,
    prompt: prompt.substring(0, 100) + '...',
    options,
  });
}

// æ€§èƒ½ç›‘æ§
const startTime = Date.now();
const result = await generateText(/* ... */);
const duration = Date.now() - startTime;

if (debugMode) {
  console.log('AI Response:', {
    duration: `${duration}ms`,
    tokens: result.usage?.totalTokens,
    cost: calculateCost(result.usage),
  });
}
```

é€šè¿‡æœ¬æ–‡æ¡£ï¼Œä½ åº”è¯¥èƒ½å¤Ÿåœ¨ Vibe Template ä¸­å……åˆ†åˆ©ç”¨ Vercel AI SDK çš„å¼ºå¤§åŠŸèƒ½ã€‚è®°ä½å§‹ç»ˆå…³æ³¨å®‰å…¨æ€§ã€æˆæœ¬æ§åˆ¶å’Œç”¨æˆ·ä½“éªŒã€‚å¦‚éœ€æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ [Vercel AI SDK å®˜æ–¹æ–‡æ¡£](https://sdk.vercel.ai/docs)ã€‚